\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Confusion matrix. Source: \citep {kohavi:glossary}.\relax }}{10}{figure.caption.43}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Examples of classifiers evaluation.\relax }}{11}{figure.caption.45}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A decision tree for decision making about playing tennis on the day.\relax }}{12}{figure.caption.49}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Source: \citep {FeiFei-2016}\relax }}{13}{figure.caption.52}
\contentsline {figure}{\numberline {3.5}{\ignorespaces A rolled (on the left) and unrolled (on the right) part of vanilla (or conventional) RNN. At each time step $t$ the model takes the $t^{th}$ member of sequence $x_t$ and outputs a hidden state value $h_t$. Source: \citep {Olah-2015}\relax }}{13}{figure.caption.54}
\contentsline {figure}{\numberline {3.6}{\ignorespaces Country and Capital Vectors Projected by PCA (Principal Component Analysis). The figure illustrates the ability of the word2vec model to organize concepts and learn the relationships between them implicitly. No supervised information about country-capital correspondence was provided to model during learning. Source: \citep {Mikolov-NIPS2013}\relax }}{15}{figure.caption.59}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The skip-gram model. Both the input vector $x$ and the output $y$ are one-hot encoded word representations. The hidden layer is the word embedding of size $N$. Source: \citep {Weng-2017}\relax }}{16}{figure.caption.61}
\contentsline {figure}{\numberline {3.8}{\ignorespaces The CBOW model. Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer. Source: \citep {Weng-2017}\relax }}{16}{figure.caption.62}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces A seq2seq model for question answering task. Source: \citep {Britz-2016}\relax }}{21}{figure.caption.90}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Visual description of experiments.\relax }}{22}{figure.caption.93}
\addvspace {10\p@ }
\contentsline {figure}{\numberline {6.1}{\ignorespaces The difference of F1 score received for user pairs with a classification model built on combination of features (standard and based on deep learning) and on only standard features. \relax }}{29}{figure.caption.128}
\addvspace {10\p@ }
\addvspace {10\p@ }
