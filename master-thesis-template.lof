\boolfalse {citerequest}\boolfalse {citetracker}\boolfalse {pagetracker}\boolfalse {backtracker}\relax 
\babel@toc {english}{}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces Confusion matrix. Source: \citep {kohavi:glossary}.\relax }}{7}{figure.caption.28}
\contentsline {figure}{\numberline {3.2}{\ignorespaces Examples of classifiers evaluation.\relax }}{8}{figure.caption.29}
\contentsline {figure}{\numberline {3.3}{\ignorespaces A decision tree for decision making about playing tennis on the day.\relax }}{8}{figure.caption.32}
\contentsline {figure}{\numberline {3.4}{\ignorespaces A 3-layer neural network with three inputs, two hidden layers of 4 neurons each and one output layer. Source: \citep {FeiFei-2016}\relax }}{9}{figure.caption.34}
\contentsline {figure}{\numberline {3.5}{\ignorespaces A rolled (on the left) and unrolled (on the right) part of RNN. At each time step $t$ the model takes the $t^{th}$ member of sequence $x_t$ and outputs a hidden state value $h_t$. Source: \citep {Olah-2015}\relax }}{10}{figure.caption.36}
\contentsline {figure}{\numberline {3.6}{\ignorespaces The skip-gram model. Both the input vector $x$ and the output $y$ are one-hot encoded word representations. The hidden layer is the word embedding of size $N$. Source: \citep {Weng-2017}\relax }}{11}{figure.caption.40}
\contentsline {figure}{\numberline {3.7}{\ignorespaces The CBOW model. Word vectors of multiple context words are averaged to get a fixed-length vector as in the hidden layer. Source: \citep {Weng-2017}\relax }}{11}{figure.caption.41}
\contentsline {figure}{\numberline {3.8}{\ignorespaces Country and Capital Vectors Projected by PCA (Principal Component Analysis). The figure illustrates the ability of the model to organize concepts and learn the relationships between them implicitly. No supervised information about country-capital correspondence was provided to model during learning. Source: \citep {Mikolov-NIPS2013}\relax }}{12}{figure.caption.42}
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces A seq2seq model for question answering task. Source: \citep {Britz-2016}\relax }}{17}{figure.caption.66}
\contentsline {figure}{\numberline {5.2}{\ignorespaces Visual description of experiments.\relax }}{18}{figure.caption.77}
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
