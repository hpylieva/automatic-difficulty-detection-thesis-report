\chapter{Conclusions}
\label{ch:conclusions}

\section{Contribution}
In this work, we considered the task of medical words' understandability detection. This task was tackled as a multiclass classification problem, and we made the following contributions:

\begin{enumerate}[listparindent=1.5em]
    \item We broaden the methodology of working with the task by introducing two new types of cross-validation scenarios for model validation. Those scenarios are close to real-world situations:
    \begin{itemize}
        \item when having the reference annotations from only a small group of users, we want our model to predict the understandability of the same set of words for all patients. 
        \item when the reference annotations are only available for a couple of users and unfull set of possible words, and we want our model to predict whether new users understand new words.
    \end{itemize}
    
    \item For the first time for the task of words' understandability detection, we utilized FastText word embeddings as features. We found out that the embeddings solely as features are not enough for good words' categorization as they do not capture the important linguistic and non-linguistic description of words. Whereas adding FastText word embeddings to standard features results in a significant improvement of classification model's performance when generalizing for unknown users (up to 4.8 higher F1 score), but may provide a decrease in performance for user pairs with different level of health literacy. Nonetheless, we consider the improvement of model's generalization ability for most of the user pairs a positive issue as when scaling to the real-world situation it is important to be able to generalize annotations provided by a small set of users on the whole population.
    
    These results of applying FastText word embeddings for automatic words' categorization on data from three annotators were published and presented on Informatics & Data-Driven Medicine workshop\footnote{\url{http://science.lpnu.ua/iddm-2018}} \citep{Pylieva:2018}.
    
    \item Inspired by the encoder part of seq2seq models \citep{Sutskever-NIPS2014}, we implemented a novel type of embeddings and called them FrnnMUTE (French RNN Medical Understandability Text Embeddings). We found out that compared with the case of using only standard features, the combination of our FrnnMUTE with standard features substantially improve the performance of classification model for all three generalization scenarios, both by unknown users and unknown words, reaching up to 5.2 higher F1 score. We also observed that the performance of standard features with FrnnMUTE is more robust and significantly better (up to 2.9 higher F-measure in user-out vocabulary-in cross-validation) than the performance of standard features with FastText word embeddings. This indicates that FrnnMUTE captures better the specifics of words needed for identifying their understandability by different users, than FastText word embeddings.
    
    The FrnnMUTE trained as described in \ref{sec:frnnmute-learning} are available for public access on \href{https://github.com/hpylieva/FrnnMUTE}{GitHub} and can be used by anyone for medical purposes.
\end{enumerate}

\section{Future work}
We have several directions for future work:
\begin{enumerate}
    \item Currently we use existing pre-trained on Wikipedia and Web Crawl word embeddings. We assume that training words embeddings on medical data may improve their impact on the categorization results.
    
    \item After analysis of results of the application of FastText word embeddings in a categorization task, we assumed the existence of a robust nonlinear dependency between some subsets of standard features and subword-level components of FastText word embeddings. We plan to test this hypothesis in further research.
    
    \item While the annotations go forward, the annotators usually show {\it learning} progress in decoding the morphological structure of terms and their understanding \citep{Grabar-BIONLP2017}. This progress is not taken into account in the current experiments. 
\end{enumerate}
