\chapter{Conclusions}
\label{ch:conclusions}

\section{Contribution}
In this work, we considered the task of medical word understandability detection. This task was tackled as a multiclass classification problem, and we made the following contributions:

\begin{enumerate}[listparindent=1.5em]
    \item We broaden the methodology of working with the task by introducing two new types of cross-validation scenarios for model validation. Those scenarios are close to real-world situations:
    \begin{itemize}
        \item when having the reference annotations from only a small group of users, we want our model to predict the understandability of the same set of words for all patients.
        
        \item when the reference annotations are only available for a small group of users and a subset of all possible words, and we want our model to predict whether new users will understand new words.
    \end{itemize}
    
    \item For the first time, for the task of detecting French word understandability in the medical domain, we utilized FastText word embeddings as features. We found out that the embeddings solely as features are not enough for good word categorization as they do not capture the important linguistic and non-linguistic description of words (F1 score is between 69.3 and 72.3). However, adding FastText word embeddings to standard features results in a substantial improvement of classification model's performance when generalizing for unknown users: F1 score reaches 85.9 and the improvement in F1 score compared to results of classification using only standard features is up to 4.8 in absolute difference. We also found out that combining FastText word embeddings with standard features may provide a decrease in performance for user pairs with different levels of health literacy. Nonetheless, we consider the improvement of the model's generalization ability for most of the user pairs a positive issue as when scaling to the real-world situation it is important to be able to generalize annotations provided by a small set of users on the whole population.
    
    These results of applying FastText word embeddings for automatic word categorization on data from three annotators were published and presented on 1st International Workshop on Informatics \& Data-Driven Medicine\footnote{\url{http://science.lpnu.ua/iddm-2018}} \citep{Pylieva:2018}.
    
    \item Inspired by the encoder part of seq2seq models \citep{Sutskever-NIPS2014}, we implemented a novel type of embeddings and called them FrnnMUTE (French RNN Medical Understandability Text Embeddings). We found out that compared with the case of using only standard features, the combination of our FrnnMUTE with standard features substantially improves the performance of classification model for all three generalization scenarios, both by unknown users and unknown words,  providing up to 5.2 higher F1 score and reaching at maximum 87.0 F1 score for user-out vocabulary-in cross-validation (80.3 F1 score in average by user pairs for this cross-validation scenario). We also observed that the performance of standard features with FrnnMUTE is more robust and significantly better (up to 2.9 higher F-measure in user-out vocabulary-in cross-validation) than the performance of standard features with FastText word embeddings. This indicates that FrnnMUTE capture better the specifics of medical words required for identifying their understandability by different users than FastText word embeddings. 
    
    \item The combination of our FrnnMUTE with standard features slightly outperformed (by at most 1.3 F1 score in absolute difference) the results in paper \citep{Grabar-PITR2014}, work from which we aimed to proceed in this work.  Also, the maximum reached 87.0 F1 score in this work is comparable to results of the top-rank systems submitted on CWI 2018 Shared Task (discussed in \ref{sec:general-lang}) for monolingual (English) studies, although the formulations of tasks are not strictly the same.
    
    The FrnnMUTE trained as described in \ref{sec:frnnmute-learning} is available for public access at GitHub\footnote{\url{https://github.com/hpylieva/FrnnMUTE}} and can be used for scientific non-commercial purposes.
\end{enumerate}

\section{Future work}
We have several directions for future work:
\begin{enumerate}
    \item Currently, we use existing pre-trained word embeddings on Wikipedia and Web Crawl. We assume that training word embeddings on medical data may improve their impact on the categorization results.
    
    \item After an analysis of results of the application of FastText word embeddings in the categorization task, we assumed the existence of a robust nonlinear dependency between some subsets of standard features and subword-level components of FastText word embeddings. We plan to test this hypothesis in further research.
    
    \item While the annotations go forward, the annotators usually show {\it learning} progress in decoding the morphological structure of terms and their understanding \citep{Grabar-BIONLP2017}. This progress is not taken into account in the current experiments, this is the topic of our future research. 
\end{enumerate}
